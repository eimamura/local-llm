{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local LLM LangChain v1 Agent\n",
        "\n",
        "This notebook demonstrates how to wire a LangChain v1 agent to a locally hosted LLM (default: [Ollama](https://ollama.ai)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f7347",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Start your local model server (e.g., run `ollama serve` after pulling a model such as `ollama pull llama3`).\n",
        "- Install the project dependencies with `uv sync` inside a virtual environment (see README).\n",
        "- Create a `.env` file (see `.env.example`) with `MODEL_NAME` and `OLLAMA_BASE_URL`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e77d2b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies via uv from the notebook (requires uv on PATH)\n",
        "# !uv sync\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420d6d9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import platform\n",
        "from typing import Any\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_ollama import ChatOllama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c15562",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load env vars and define sane defaults for local development\n",
        "load_dotenv()\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3\")\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "TEMPERATURE = float(os.getenv(\"MODEL_TEMPERATURE\", \"0.2\"))\n",
        "print(f\"Using model={MODEL_NAME} @ {OLLAMA_BASE_URL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e796839d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the local LLM client (Ollama by default)\n",
        "llm = ChatOllama(\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    model=MODEL_NAME,\n",
        "    temperature=TEMPERATURE,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fc7864",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def system_context(request: str = \"\") -> str:\n",
        "    \"\"\"Return contextual information about the workstation (OS, time).\"\"\"\n",
        "    now = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
        "    return f\"System time: {now}\\nPlatform: {platform.platform()}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_math(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate simple math expressions using Python syntax.\"\"\"\n",
        "    allowed_globals: dict[str, Any] = {\"__builtins__\": {}}\n",
        "    try:\n",
        "        result = eval(expression, allowed_globals, {})\n",
        "    except Exception as exc:  # noqa: BLE001 - provide feedback to agent\n",
        "        return f\"error: {exc}\"\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "tools = [system_context, python_math]\n",
        "for t in tools:\n",
        "    print(f\"Loaded tool: {t.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0308c586",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = os.getenv(\n",
        "    \"SYSTEM_PROMPT\",\n",
        "    \"You are a helpful local-only AI agent. Use the available tools when they can improve the answer before responding.\",\n",
        ")\n",
        "print(\"Using LangChain 1.x tool-calling agent graph (model must support native tool calls).\")\n",
        "agent_runnable = create_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        ")\n",
        "\n",
        "def invoke_agent(user_input: str) -> str:\n",
        "    try:\n",
        "        state = agent_runnable.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]})\n",
        "    except Exception as exc:\n",
        "        message = str(exc).lower()\n",
        "        if \"does not support tools\" in message:\n",
        "            raise RuntimeError(\"The current model/runtime does not expose tool calling. Switch to a tool-capable Ollama model (e.g., llama3.1, mistral-function) or rebuild with tool support.\") from exc\n",
        "        raise\n",
        "    return state[\"messages\"][-1].content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7565861b",
      "metadata": {},
      "source": [
        "## Run the agent\n",
        "\n",
        "Call `invoke_agent(...)` with a user prompt. Make sure your Ollama/LLM runtime exposes tool calling; otherwise create_agent() will raise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4313b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What time is it?\"\n",
        "invoke_agent(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183e59e9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
