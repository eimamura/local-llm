{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local LLM LangChain v1 Agent\n",
        "\n",
        "This notebook demonstrates how to wire a LangChain v1 agent to a locally hosted LLM (default: [Ollama](https://ollama.ai)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Start your local model server (e.g., run `ollama serve` after pulling a model such as `ollama pull llama3`).\n",
        "- Install the project dependencies with `pip install -r requirements.txt` inside a virtual environment.\n",
        "- Create a `.env` file (see `.env.example`) with `MODEL_NAME` and `OLLAMA_BASE_URL`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies directly from the notebook (uncomment if needed)\n",
        "# %pip install -r ../requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import platform\n",
        "from typing import Any\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.tools import tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load env vars and define sane defaults for local development\n",
        "load_dotenv()\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3\")\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "TEMPERATURE = float(os.getenv(\"MODEL_TEMPERATURE\", \"0.2\"))\n",
        "print(f\"Using model={MODEL_NAME} @ {OLLAMA_BASE_URL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the local LLM client (Ollama by default)\n",
        "llm = ChatOllama(\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    model=MODEL_NAME,\n",
        "    temperature=TEMPERATURE,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def system_context(_: str = \"\") -> str:\n",
        "    \"\"\"Return contextual information about the workstation (OS, time).\"\"\"\n",
        "    now = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
        "    return f\"System time: {now}\\nPlatform: {platform.platform()}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_math(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate simple math expressions using Python syntax.\"\"\"\n",
        "    allowed_globals: dict[str, Any] = {\"__builtins__\": {}}\n",
        "    try:\n",
        "        result = eval(expression, allowed_globals, {})\n",
        "    except Exception as exc:  # noqa: BLE001 - provide feedback to agent\n",
        "        return f\"error: {exc}\"\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "tools = [system_context, python_math]\n",
        "for t in tools:\n",
        "    print(f\"Loaded tool: {t.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful local-only AI agent. Use the available tools when helpful before responding.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the agent\n",
        "\n",
        "Use `agent_executor.invoke({\"input\": \"<prompt>\"})` to get a structured response. The `output` key carries the final answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What's the current system context and what is (42 * 17) - 5?\"\n",
        "result = agent_executor.invoke({\"input\": query})\n",
        "result[\"output\"]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}